{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../dataset.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/py3.10-research/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/py3.10-research/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/py3.10-research/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/py3.10-research/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.10/envs/py3.10-research/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature and Target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Result', axis=1)\n",
    "y = df['Result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and K-Fold Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Storage for Each Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies, precisions, recalls, f1_scores = [], [], [], []\n",
    "tpr_list, fpr_list, tnr_list, fnr_list = [], [], [], []\n",
    "tp_list, fp_list, tn_list, fn_list = [], [], [], []\n",
    "total_list = []\n",
    "# Lists to store training and testing times\n",
    "train_times = []\n",
    "test_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing and Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "importance_list = []\n",
    "\n",
    "SELECT_K = 25\n",
    "\n",
    "# Scaling data dilakukan di luar K-Fold (sebelum looping)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    # Split data yang sudah di-scaling sebelumnya\n",
    "    X_train_scaled, X_test_scaled = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Hitung F-score dan p-value\n",
    "    f_scores, p_values = f_classif(X_train_scaled, y_train)\n",
    "\n",
    "    # Pilih index fitur dengan p-value terkecil\n",
    "    selected_indices = np.argsort(p_values)[:SELECT_K]\n",
    "    selected_feature_names = feature_names[selected_indices]\n",
    "    selected_f_scores = f_scores[selected_indices]\n",
    "\n",
    "    # Seleksi fitur berdasarkan p-value\n",
    "    X_train_selected = X_train_scaled[:, selected_indices]\n",
    "    X_test_selected = X_test_scaled[:, selected_indices]\n",
    "\n",
    "    # Training dan ukur waktu training\n",
    "    train_start_time = time.time()\n",
    "    rf_model.fit(X_train_selected, y_train)\n",
    "    train_end_time = time.time()\n",
    "    train_time = train_end_time - train_start_time\n",
    "    train_times.append(train_time)\n",
    "\n",
    "    # Testing dan ukur waktu testing\n",
    "    test_start_time = time.time()\n",
    "    y_pred = rf_model.predict(X_test_selected)\n",
    "    test_end_time = time.time()\n",
    "    test_time = test_end_time - test_start_time\n",
    "    test_times.append(test_time)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[-1, 1])\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    total = TP + TN + FP + FN\n",
    "\n",
    "    # Simpan nilai absolut\n",
    "    tp_list.append(TP)\n",
    "    fp_list.append(FP)\n",
    "    tn_list.append(TN)\n",
    "    fn_list.append(FN)\n",
    "    total_list.append(total)\n",
    "\n",
    "    # Simpan nilai metrik\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    precisions.append(precision_score(y_test, y_pred, pos_label=1, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred, pos_label=1, zero_division=0))\n",
    "    f1_scores.append(f1_score(y_test, y_pred, pos_label=1, zero_division=0))\n",
    "\n",
    "    tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    fpr = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    tnr = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    fnr = FN / (FN + TP) if (FN + TP) > 0 else 0\n",
    "\n",
    "    tpr_list.append(tpr)\n",
    "    fpr_list.append(fpr)\n",
    "    tnr_list.append(tnr)\n",
    "    fnr_list.append(fnr)\n",
    "\n",
    "    # Simpan feature importance dari fitur terpilih\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': selected_feature_names,\n",
    "        'Importance': selected_f_scores\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    importance_list.append(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Absolute Average Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_tp, avg_fp, avg_tn, avg_fn = np.mean(tp_list), np.mean(fp_list), np.mean(tn_list), np.mean(fn_list)\n",
    "avg_total = np.mean(total_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Fold Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Evaluation Results (All in Percentage) ---\")\n",
    "print(\"General Metrics:\")\n",
    "print(f\"  Average Accuracy    : {np.mean(accuracies) * 100:.2f}%\")\n",
    "print(f\"  Average Precision   : {np.mean(precisions) * 100:.2f}%\")\n",
    "print(f\"  Average Recall      : {np.mean(recalls) * 100:.2f}%\")\n",
    "print(f\"  Average F1-Score    : {np.mean(f1_scores) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nExecution Time:\")\n",
    "print(f\"  Average Training Time : {np.mean(train_times):.4f} seconds\")\n",
    "print(f\"  Average Testing Time  : {np.mean(test_times):.4f} seconds\")\n",
    "print(f\"  Total Training Time   : {np.sum(train_times):.4f} seconds\")\n",
    "print(f\"  Total Testing Time    : {np.sum(test_times):.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Confusion Matrix per Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAverage Confusion Matrix per Fold (absolute and percentage):\")\n",
    "print(f\"  TP: {avg_tp:.2f} ({avg_tp / avg_total * 100:.2f}%)\")\n",
    "print(f\"  FP: {avg_fp:.2f} ({avg_fp / avg_total * 100:.2f}%)\")\n",
    "print(f\"  TN: {avg_tn:.2f} ({avg_tn / avg_total * 100:.2f}%)\")\n",
    "print(f\"  FN: {avg_fn:.2f} ({avg_fn / avg_total * 100:.2f}%)\")\n",
    "\n",
    "# Calculate FPR\n",
    "fpr = avg_fp / (avg_fp + avg_tn)\n",
    "print(f\"\\nFalse Positive Rate (FPR): {fpr:.4f} ({fpr * 100:.2f}%)\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Feature Importance Across All Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_importance = pd.concat(importance_list)\n",
    "mean_importance = all_importance.groupby('Feature')['Importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\n--- Average Feature Importance from 10 Folds (Top {SELECT_K}) ---\")\n",
    "print(mean_importance.head(SELECT_K).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Top 30 Feature Importance Vertical Bar Chart (Similar to provided image)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get top 30 features\n",
    "top_30_features = mean_importance.head(30)\n",
    "\n",
    "# Create figure with larger size to accommodate 30 features\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Create vertical bar chart with alternating colors (blue and purple like in the image)\n",
    "colors = ['#4472C4' if i % 2 == 0 else '#8A2BE2' for i in range(len(top_30_features))]\n",
    "bars = plt.bar(range(len(top_30_features)), top_30_features.values, \n",
    "               color=colors, alpha=0.8, edgecolor='white', linewidth=0.5)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i, (feature, importance) in enumerate(top_30_features.items()):\n",
    "    plt.text(i, importance + importance*0.01, f'{importance:.2f}', \n",
    "             ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Top 30 Feature Importance (Vertical Bar Chart)', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Features', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('F-Score (Feature Importance)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Set x-axis labels with rotation\n",
    "plt.xticks(range(len(top_30_features)), top_30_features.index, \n",
    "           rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Set y-axis to start from 0 for better comparison\n",
    "plt.ylim(0, top_30_features.iloc[0] * 1.1)\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Print the top 30 values for reference\n",
    "print(\"\\n--- Top 30 Feature Importance Values ---\")\n",
    "for i, (feature, importance) in enumerate(top_30_features.items(), 1):\n",
    "    print(f\"{i:2d}. {feature}: {importance:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avg_cm = np.array([[avg_tn, avg_fp],\n",
    "                   [avg_fn, avg_tp]])\n",
    "\n",
    "labels = ['Negative (-1)', 'Positive (1)']\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(avg_cm, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "plt.title(\"Average Confusion Matrix per Fold\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution Time Visualization per Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of training and testing time per fold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "folds = list(range(1, 11))  # 10 folds from CV\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(folds, train_times, color='royalblue', alpha=0.7)\n",
    "plt.axhline(y=np.mean(train_times), color='red', linestyle='--', label=f'Mean: {np.mean(train_times):.4f}s')\n",
    "plt.title('Training Time per Fold', fontweight='bold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Plot testing time\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(folds, test_times, color='forestgreen', alpha=0.7)\n",
    "plt.axhline(y=np.mean(test_times), color='red', linestyle='--', label=f'Mean: {np.mean(test_times):.4f}s')\n",
    "plt.title('Testing Time per Fold', fontweight='bold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pie chart comparing training and testing time\n",
    "plt.figure(figsize=(8, 8))\n",
    "total_time = np.sum(train_times) + np.sum(test_times)\n",
    "labels = ['Training Time', 'Testing Time']\n",
    "sizes = [np.sum(train_times)/total_time*100, np.sum(test_times)/total_time*100]\n",
    "colors = ['royalblue', 'forestgreen']\n",
    "explode = (0.1, 0)  # explode the 1st slice (Training)\n",
    "\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('Percentage of Training vs Testing Time', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison of testing time between 25 and 29 features\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Save data for 25 features\n",
    "test_times_25 = test_times.copy()\n",
    "mean_test_time_25 = np.mean(test_times_25)\n",
    "\n",
    "# Run model with 29 features for comparison\n",
    "SELECT_K_29 = 29\n",
    "test_times_29 = []\n",
    "\n",
    "# Use the same k-fold as before\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "    # Split the already scaled data\n",
    "    X_train_scaled, X_test_scaled = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Calculate F-score and p-value\n",
    "    f_scores, p_values = f_classif(X_train_scaled, y_train)\n",
    "\n",
    "    # Select feature indices with smallest p-values\n",
    "    selected_indices_29 = np.argsort(p_values)[:SELECT_K_29]\n",
    "\n",
    "    # Feature selection based on p-value\n",
    "    X_train_selected_29 = X_train_scaled[:, selected_indices_29]\n",
    "    X_test_selected_29 = X_test_scaled[:, selected_indices_29]\n",
    "\n",
    "    # Train model with 29 features\n",
    "    rf_model_29 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model_29.fit(X_train_selected_29, y_train)\n",
    "\n",
    "    # Test and measure testing time for 29 features\n",
    "    test_start_time = time.time()\n",
    "    y_pred_29 = rf_model_29.predict(X_test_selected_29)\n",
    "    test_end_time = time.time()\n",
    "    test_time_29 = test_end_time - test_start_time\n",
    "    test_times_29.append(test_time_29)\n",
    "\n",
    "# Calculate average testing time for 29 features\n",
    "mean_test_time_29 = np.mean(test_times_29)\n",
    "\n",
    "# Visualization comparing testing time between 25 and 29 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Per-fold comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "x = np.arange(1, 11)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = plt.bar(x - width/2, test_times_25, width, label='25 Features', color='skyblue')\n",
    "bars2 = plt.bar(x + width/2, test_times_29, width, label='29 Features', color='salmon')\n",
    "\n",
    "plt.xlabel('Fold', fontsize=12)\n",
    "plt.ylabel('Testing Time (seconds)', fontsize=12)\n",
    "plt.title('Comparison of Testing Time per Fold: 25 vs 29 Features', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "# Average testing time\n",
    "plt.subplot(2, 1, 2)\n",
    "avg_times = [mean_test_time_25, mean_test_time_29]\n",
    "labels = ['25 Features', '29 Features']\n",
    "colors = ['skyblue', 'salmon']\n",
    "\n",
    "bars = plt.bar(labels, avg_times, color=colors)\n",
    "plt.ylabel('Average Testing Time (seconds)', fontsize=12)\n",
    "plt.title('Comparison of Average Testing Time: 25 vs 29 Features', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add values above bars\n",
    "for i, v in enumerate(avg_times):\n",
    "    plt.text(i, v + 0.0001, f'{v:.5f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# Add percentage change\n",
    "percent_change = ((mean_test_time_29 - mean_test_time_25) / mean_test_time_25) * 100\n",
    "increase_text = f'Increase: {percent_change:.2f}%' if percent_change > 0 else f'Decrease: {abs(percent_change):.2f}%'\n",
    "plt.figtext(0.5, 0.3, increase_text, ha='center', fontsize=12, fontweight='bold', \n",
    "           bbox={'facecolor':'lightgreen', 'alpha':0.5, 'pad':5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\n--- Testing Time Comparison: 25 vs 29 Features ---\")\n",
    "print(f\"Average Testing Time (25 Features): {mean_test_time_25:.5f} seconds\")\n",
    "print(f\"Average Testing Time (29 Features): {mean_test_time_29:.5f} seconds\")\n",
    "print(f\"Absolute Difference: {abs(mean_test_time_29 - mean_test_time_25):.5f} seconds\")\n",
    "print(f\"Relative Difference: {percent_change:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization: Horizontal bar chart for clearer comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Data\n",
    "categories = ['25 Features', '29 Features']\n",
    "values = [mean_test_time_25, mean_test_time_29]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "# Plot horizontal bar chart\n",
    "bars = plt.barh(categories, values, color=colors, height=0.5)\n",
    "\n",
    "# Add values at the end of bars\n",
    "for i, v in enumerate(values):\n",
    "    plt.text(v + v*0.01, i, f'{v:.6f}s', va='center', fontweight='bold')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Comparison of Average Testing Time (seconds)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Time (seconds)', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add grid for easier reading\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add percentage change information\n",
    "percent_change_text = f'Increase: {percent_change:.2f}%' if percent_change > 0 else f'Decrease: {abs(percent_change):.2f}%'\n",
    "plt.figtext(0.5, 0.01, percent_change_text, ha='center', fontsize=12, fontweight='bold', \n",
    "           bbox={'facecolor':'lightgreen', 'alpha':0.5, 'pad':5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization: Bar chart for absolute time comparison with 5 test runs\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Run the test 5 times for each feature set and calculate average\n",
    "n_runs = 5\n",
    "test_times_25_multiple = []\n",
    "test_times_29_multiple = []\n",
    "\n",
    "print(f\"\\n--- Running {n_runs} test iterations for more reliable timing ---\")\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"Run {run+1}/{n_runs}...\")\n",
    "    \n",
    "    # Test times for 25 features (run multiple times)\n",
    "    run_times_25 = []\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_test_selected = X_scaled[test_idx][:, np.argsort(p_values)[:SELECT_K]]\n",
    "        \n",
    "        # Measure testing time\n",
    "        test_start_time = time.time()\n",
    "        _ = rf_model.predict(X_test_selected)\n",
    "        test_end_time = time.time()\n",
    "        run_times_25.append(test_end_time - test_start_time)\n",
    "    \n",
    "    # Test times for 29 features (run multiple times)\n",
    "    run_times_29 = []\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y)):\n",
    "        X_test_selected_29 = X_scaled[test_idx][:, np.argsort(p_values)[:SELECT_K_29]]\n",
    "        \n",
    "        # Measure testing time\n",
    "        test_start_time = time.time()\n",
    "        _ = rf_model_29.predict(X_test_selected_29)\n",
    "        test_end_time = time.time()\n",
    "        run_times_29.append(test_end_time - test_start_time)\n",
    "    \n",
    "    # Save average times for this run\n",
    "    test_times_25_multiple.append(np.mean(run_times_25))\n",
    "    test_times_29_multiple.append(np.mean(run_times_29))\n",
    "\n",
    "# Calculate final average over all runs\n",
    "mean_test_time_25_multi = np.mean(test_times_25_multiple)\n",
    "mean_test_time_29_multi = np.mean(test_times_29_multiple)\n",
    "\n",
    "# Print detailed results of multiple runs\n",
    "print(\"\\n--- Testing Time Comparison (Average of 5 runs) ---\")\n",
    "print(f\"25 Features - Individual run means: {[f'{t:.6f}s' for t in test_times_25_multiple]}\")\n",
    "print(f\"29 Features - Individual run means: {[f'{t:.6f}s' for t in test_times_29_multiple]}\")\n",
    "print(f\"25 Features - Final average: {mean_test_time_25_multi:.6f} seconds\")\n",
    "print(f\"29 Features - Final average: {mean_test_time_29_multi:.6f} seconds\")\n",
    "\n",
    "# Calculate absolute difference\n",
    "absolute_diff = mean_test_time_29_multi - mean_test_time_25_multi\n",
    "\n",
    "# Data for bar chart\n",
    "labels = ['25 Features', '29 Features']\n",
    "values = [mean_test_time_25_multi, mean_test_time_29_multi]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(labels, values, color=colors, width=0.6)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.00002,\n",
    "            f'{height:.6f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Comparison of Absolute Testing Time (Average of 5 Runs)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Time (seconds)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add absolute time difference as text annotation\n",
    "diff_text = f\"Absolute Time Difference: {abs(absolute_diff):.6f} seconds\"\n",
    "plt.figtext(0.5, 0.01, diff_text, ha='center', fontsize=12, fontweight='bold', \n",
    "           bbox={'facecolor':'lightblue', 'alpha':0.5, 'pad':5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create horizontal bar chart for clearer visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Horizontal bar chart\n",
    "bars_h = plt.barh(labels, values, color=colors, height=0.5)\n",
    "\n",
    "# Add value labels at the end of bars\n",
    "for i, bar in enumerate(bars_h):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.00002, i, f'{width:.6f}s', va='center', ha='left', fontweight='bold')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Comparison of Absolute Testing Time (Average of 5 Runs)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Time (seconds)', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add absolute time difference as text annotation\n",
    "plt.figtext(0.5, 0.01, diff_text, ha='center', fontsize=12, fontweight='bold', \n",
    "           bbox={'facecolor':'lightblue', 'alpha':0.5, 'pad':5})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Model to ONNX\n",
    "import skl2onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Define input type (None means variable batch size, 25 is number of features)\n",
    "# Model 'rf_model' dilatih dengan 25 fitur terpilih\n",
    "initial_type = [('float_input', FloatTensorType([None, 25]))]\n",
    "\n",
    "# Convert the model (using rf_model from the training loop)\n",
    "onx = convert_sklearn(rf_model, initial_types=initial_type)\n",
    "\n",
    "# Save the model\n",
    "with open(\"rf_model.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())\n",
    "\n",
    "print(\"Model exported to rf_model.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
